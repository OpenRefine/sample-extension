{
  "llm-extension/manage-llm": "Manage LLM Providers",
  "llm-extension/menu-label": "AI",
  "llm-management/llm-list-header": "LLM providers",
  "llm-management/llm-help": "This extension supports the openai chat completion model and can be defined and used in any project.",
  "llm-management/close":"Close",
  "llm-management/add-llm": "Add LLM Provider",
  "llm-management/info": "This extension supports Chat Completion API services (/v1/chat/completions) from various LLM providers. See our provider setup guide for configuration details, parameter documentation for optimizing your prompts and results. ",
  "llm-nanagement/provider-guide": "LLM Provider setup guide",
  "llm-management/feature-guide": "Feature working guide",
  "llm-detail/dialog-header": "LLM Provider",
  "llm-detail/label": "Label",
  "llm-detail/model": "Model",
  "llm-detail/serverUrl": "Server URL",
  "llm-detail/temperature": "Temperature",
  "llm-detail/topP": "Top-P",
  "llm-detail/seed": "Seed",
  "llm-detail/maxTokens": "Max tokens",
  "llm-detail/apikey": "API Key",
  "llm-detail/waitTime": "Wait Time (milliseconds)",
  "llm-detail/test": "Test service",
  "llm-detail/save": "Save",
  "llm-detail/cancel": "Cancel",
  "llm-detail/response": "Response",
  "llm-detail/actions": "Actions",
  "llm-detail/provider-missing-info": "Label, Server Url, Model, API Key are required.",
  "llm-detail/provider-temperature-error": "Temperature setting has to be between 0 & 2",
  "llm-detail/provider-maxtokens-error": "Max tokens cannot be 0",
  "llm-detail/provider-waittime-error": "Waittime must be set to 0 for no wait between requests, or any valid non-zero value to specify a wait duration.",
  "llm-detail/saving": "Saving ...",
  "llm-details/deleting": "Deleting ...",
  "llm-details/confirm-delete": "Are you sure you want to delete the LLM Provider?",
  "llm-extension/chat": "Extract using AI",
  "llm-chatcompletion/add-by-llm": "Extract using AI from column ",
  "llm-chatcompletion/new-col-name": "Column",
  "llm-chatcompletion/col-mode-add": "Add new",
  "llm-chatcompletion/col-mode-upd": "Update existing",
  "llm-chatcompletion/response-format-text": "Text",
  "llm-chatcompletion/response-format-json-schema": "JSON as per schema",
  "llm-chatcompletion/response-format-json-object": "JSON object",
  "llm-chatcompletion/llm-selector": "LLM Provider",
  "llm-chatcompletion/responseformat-selector": "Response format",
  "llm-chatcompletion/system-prompt": "Describe what needs to be done",
  "llm-chatcompletion/json-schema": "Provide JSON Schema",
  "llm-chatcompletion/json-schema-hint": "Enabled when Response Format is set to JSON Schema",
  "llm-chatcompletion/preview": "Generate Preview",
  "llm-chatcompletion/preview-response": "response",
  "llm-chatcompletion/preview-request": "value",
  "llm-chatcompletion/response-help-text": "AI-generated response will appear here",
  "llm-chatcompletion/preview-label": "Preview",
  "llm-chatcompletion/history-label": "History",
  "llm-chatcompletion/starred-label": "Starred",
  "llm-chatcompletion/prompt-help": "Help",
  "llm-extension/processing": "Processing LLM request ...",
  "llm-chatcompletion/select-one": "Select one",
  "llm-chatcompletion/preview-missing-info": "Preview requires LLM Provider, Response format & Task detail to be set.",
  "llm-chatconpletion/preview-missing-schema": "JSON schema is required",
  "llm-chatcompletion/history-action": "Action",
  "llm-chatcompletion/history-source": "Source",
  "llm-chatcompletion/history-reuse": "Reuse",
  "llm-chatcompletion/history-starred": "Star",
  "llm-chatcompletion/history-json-schema": "JSON Schema",
  "llm-chatcompletion/history-prompt": "Prompt",
  "llm-chatcompletion/this-project": "This Project",
  "llm-chatcompletion/other-project": "Other Project",
  "llm-chatcompletion/star-hint": "Star the Prompt",
  "llm-chatcompletion/unstar-hint": "Unstar the Prompt"
}