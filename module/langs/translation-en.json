{
  "llm-extension/manage-llm": "Manage LLM Providers",
  "llm-extension/menu-label": "AI",
  "llm-management/llm-list-header": "LLM providers",
  "llm-management/llm-help": "This extension supports the openai chat completion model and can be defined and used in any project.",
  "llm-management/close":"Close",
  "llm-management/add-llm": "Add LLM Provider",
  "llm-management/info": "This extension supports Chat Completion API services (/v1/chat/completions) from various LLM providers. See our provider setup guide for configuration details, parameter documentation for optimizing your prompts and results. ",
  "llm-nanagement/provider-guide": "LLM Provider setup guide",
  "llm-management/feature-guide": "Feature working guide",
  "llm-detail/dialog-header": "LLM Provider",
  "llm-detail/label": "Label",
  "llm-detail/model": "Model",
  "llm-detail/serverUrl": "Server URL",
  "llm-detail/temperature": "Temperature",
  "llm-detail/topP": "Top-P",
  "llm-detail/seed": "Seed",
  "llm-detail/maxTokens": "Max tokens",
  "llm-detail/apikey": "API Key",
  "llm-detail/test": "Test service",
  "llm-detail/save": "Save",
  "llm-detail/cancel": "Cancel",
  "llm-detail/response": "Response",
  "llm-detail/actions": "Actions",
  "llm-detail/provider-missing-info": "Label, Server Url, Model, API Key are required.",
  "llm-detail/provider-temperature-error": "Temperature setting has to be between 0 & 2",
  "llm-detail/provider-maxtokens-error": "Max tokens cannot be 0",
  "llm-detail/saving": "Saving ...",
  "llm-details/deleting": "Deleting ...",
  "llm-details/confirm-delete": "Are you sure you want to delete the LLM Provider?",
  "llm-extension/chat": "Extract using AI",
  "llm-chatcompletion/add-by-llm": "Extract using AI from column ",
  "llm-chatcompletion/new-col-name": "New Column name",
  "llm-chatcompletion/llm-selector": "LLM Provider",
  "llm-chatcompletion/responseformat-selector": "Response format",
  "llm-chatcompletion/system-prompt": "Describe what needs to be done",
  "llm-chatcompletion/json-schema": "Provide JSON Schema",
  "llm-chatcompletion/json-schema-hint": "Enabled when Response Format is set to JSON Schema",
  "llm-chatcompletion/preview": "Generate Preview",
  "llm-chatcompletion/preview-response": "response",
  "llm-chatcompletion/preview-request": "value",
  "llm-chatcompletion/response-help-text": "AI-generated response will appear here",
  "llm-chatcompletion/preview-label": "Preview",
  "llm-chatcompletion/prompt-help": "Help",
  "llm-extension/processing": "Processing LLM request ...",
  "llm-chatcompletion/select-one": "Select one",
  "llm-chatcompletion/preview-missing-info": "Preview requires LLM Provider, Response format & Task detail to be set.",
  "llm-chatconpletion/preview-missing-schema": "JSON schema is required"
}